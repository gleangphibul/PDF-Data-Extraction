{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzJ5cWg1Iq7Bco/qVjDBK4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gleangphibul/PDF-Data-Extraction/blob/main/Garn_PDFTextExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Author: Phuwadol (Garn) Leangphibul"
      ],
      "metadata": {
        "id": "DtqWygRpr4DZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fwbp7wKunL2V",
        "outputId": "8c0b74eb-572f-48cd-814c-839f599fddc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pikepdf==8.3.0\n",
            "  Downloading pikepdf-8.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: Pillow>=9.0 in /usr/local/lib/python3.12/dist-packages (from pikepdf==8.3.0) (11.3.0)\n",
            "Collecting deprecation (from pikepdf==8.3.0)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: lxml>=4.8 in /usr/local/lib/python3.12/dist-packages (from pikepdf==8.3.0) (5.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pikepdf==8.3.0) (25.0)\n",
            "Downloading pikepdf-8.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: deprecation, pikepdf\n",
            "Successfully installed deprecation-2.1.0 pikepdf-8.3.0\n",
            "Collecting ocrmypdf\n",
            "  Downloading ocrmypdf-16.11.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: deprecation>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from ocrmypdf) (2.1.0)\n",
            "Collecting img2pdf>=0.5 (from ocrmypdf)\n",
            "  Downloading img2pdf-0.6.1.tar.gz (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from ocrmypdf) (25.0)\n",
            "Collecting pdfminer-six>=20220319 (from ocrmypdf)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting pi-heif (from ocrmypdf)\n",
            "  Downloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting pikepdf!=9.8.0,>=8.10.1 (from ocrmypdf)\n",
            "  Downloading pikepdf-10.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: pillow>=10.0.1 in /usr/local/lib/python3.12/dist-packages (from ocrmypdf) (11.3.0)\n",
            "Requirement already satisfied: pluggy>=1 in /usr/local/lib/python3.12/dist-packages (from ocrmypdf) (1.6.0)\n",
            "Requirement already satisfied: rich>=13 in /usr/local/lib/python3.12/dist-packages (from ocrmypdf) (13.9.4)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer-six>=20220319->ocrmypdf) (3.4.4)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer-six>=20220319->ocrmypdf) (43.0.3)\n",
            "Collecting Deprecated (from pikepdf!=9.8.0,>=8.10.1->ocrmypdf)\n",
            "  Downloading deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: lxml>=4.8 in /usr/local/lib/python3.12/dist-packages (from pikepdf!=9.8.0,>=8.10.1->ocrmypdf) (5.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13->ocrmypdf) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13->ocrmypdf) (2.19.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer-six>=20220319->ocrmypdf) (2.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13->ocrmypdf) (0.1.2)\n",
            "Requirement already satisfied: wrapt<3,>=1.10 in /usr/local/lib/python3.12/dist-packages (from Deprecated->pikepdf!=9.8.0,>=8.10.1->ocrmypdf) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six>=20220319->ocrmypdf) (2.23)\n",
            "Downloading ocrmypdf-16.11.1-py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.0/163.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pikepdf-10.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecated-1.3.1-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: img2pdf\n",
            "  Building wheel for img2pdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for img2pdf: filename=img2pdf-0.6.1-py3-none-any.whl size=51001 sha256=66af75d92de9ffcee9c7615575943b996e7d7b76906c84c7fdabdd1fd4ff25b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/05/56/c05447973db749cd2178b8f95e36f007f0af5f5dce2c6197a5\n",
            "Successfully built img2pdf\n",
            "Installing collected packages: pi-heif, Deprecated, pikepdf, pdfminer-six, img2pdf, ocrmypdf\n",
            "  Attempting uninstall: pikepdf\n",
            "    Found existing installation: pikepdf 8.3.0\n",
            "    Uninstalling pikepdf-8.3.0:\n",
            "      Successfully uninstalled pikepdf-8.3.0\n",
            "Successfully installed Deprecated-1.3.1 img2pdf-0.6.1 ocrmypdf-16.11.1 pdfminer-six-20250506 pi-heif-1.1.1 pikepdf-10.0.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libarchive-dev libleptonica-dev\n",
            "The following NEW packages will be installed:\n",
            "  libarchive-dev libleptonica-dev libtesseract-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 3,743 kB of archives.\n",
            "After this operation, 16.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libarchive-dev amd64 3.6.0-1ubuntu1.5 [581 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libleptonica-dev amd64 1.82.0-3build1 [1,562 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libtesseract-dev amd64 4.1.1-2.1build1 [1,600 kB]\n",
            "Fetched 3,743 kB in 1s (4,846 kB/s)\n",
            "Selecting previously unselected package libarchive-dev:amd64.\n",
            "(Reading database ... 125081 files and directories currently installed.)\n",
            "Preparing to unpack .../libarchive-dev_3.6.0-1ubuntu1.5_amd64.deb ...\n",
            "Unpacking libarchive-dev:amd64 (3.6.0-1ubuntu1.5) ...\n",
            "Selecting previously unselected package libleptonica-dev.\n",
            "Preparing to unpack .../libleptonica-dev_1.82.0-3build1_amd64.deb ...\n",
            "Unpacking libleptonica-dev (1.82.0-3build1) ...\n",
            "Selecting previously unselected package libtesseract-dev:amd64.\n",
            "Preparing to unpack .../libtesseract-dev_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Setting up libleptonica-dev (1.82.0-3build1) ...\n",
            "Setting up libarchive-dev:amd64 (3.6.0-1ubuntu1.5) ...\n",
            "Setting up libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (11.3.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-droid-fallback fonts-noto-mono fonts-urw-base35 libgs9 libgs9-common\n",
            "  libidn12 libijs-0.35 libjbig2dec0 poppler-data\n",
            "Suggested packages:\n",
            "  fonts-noto fonts-freefont-otf | fonts-freefont-ttf fonts-texgyre\n",
            "  ghostscript-x poppler-utils fonts-japanese-mincho | fonts-ipafont-mincho\n",
            "  fonts-japanese-gothic | fonts-ipafont-gothic fonts-arphic-ukai\n",
            "  fonts-arphic-uming fonts-nanum\n",
            "The following NEW packages will be installed:\n",
            "  fonts-droid-fallback fonts-noto-mono fonts-urw-base35 ghostscript libgs9\n",
            "  libgs9-common libidn12 libijs-0.35 libjbig2dec0 poppler-data\n",
            "0 upgraded, 10 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 16.7 MB of archives.\n",
            "After this operation, 63.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1build1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 poppler-data all 0.4.11-1 [2,171 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-mono all 20201225-1build1 [397 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-urw-base35 all 20200910-1 [6,367 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9-common all 9.55.0~dfsg1-0ubuntu5.13 [753 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libidn12 amd64 1.38-4ubuntu1 [60.0 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libijs-0.35 amd64 0.35-15build2 [16.5 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjbig2dec0 amd64 0.19-3build2 [64.7 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9 amd64 9.55.0~dfsg1-0ubuntu5.13 [5,032 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ghostscript amd64 9.55.0~dfsg1-0ubuntu5.13 [49.4 kB]\n",
            "Fetched 16.7 MB in 2s (8,339 kB/s)\n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 125214 files and directories currently installed.)\n",
            "Preparing to unpack .../0-fonts-droid-fallback_1%3a6.0.1r16-1.1build1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../1-poppler-data_0.4.11-1_all.deb ...\n",
            "Unpacking poppler-data (0.4.11-1) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../2-fonts-noto-mono_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-mono (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-urw-base35.\n",
            "Preparing to unpack .../3-fonts-urw-base35_20200910-1_all.deb ...\n",
            "Unpacking fonts-urw-base35 (20200910-1) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../4-libgs9-common_9.55.0~dfsg1-0ubuntu5.13_all.deb ...\n",
            "Unpacking libgs9-common (9.55.0~dfsg1-0ubuntu5.13) ...\n",
            "Selecting previously unselected package libidn12:amd64.\n",
            "Preparing to unpack .../5-libidn12_1.38-4ubuntu1_amd64.deb ...\n",
            "Unpacking libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../6-libijs-0.35_0.35-15build2_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../7-libjbig2dec0_0.19-3build2_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../8-libgs9_9.55.0~dfsg1-0ubuntu5.13_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.13) ...\n",
            "Selecting previously unselected package ghostscript.\n",
            "Preparing to unpack .../9-ghostscript_9.55.0~dfsg1-0ubuntu5.13_amd64.deb ...\n",
            "Unpacking ghostscript (9.55.0~dfsg1-0ubuntu5.13) ...\n",
            "Setting up fonts-noto-mono (20201225-1build1) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Setting up fonts-urw-base35 (20200910-1) ...\n",
            "Setting up poppler-data (0.4.11-1) ...\n",
            "Setting up libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Setting up libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Setting up libgs9-common (9.55.0~dfsg1-0ubuntu5.13) ...\n",
            "Setting up libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.13) ...\n",
            "Setting up ghostscript (9.55.0~dfsg1-0ubuntu5.13) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting tabula-py\n",
            "  Downloading tabula_py-2.10.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.12/dist-packages (from tabula-py) (2.2.2)\n",
            "Requirement already satisfied: numpy>1.24.4 in /usr/local/lib/python3.12/dist-packages (from tabula-py) (2.0.2)\n",
            "Requirement already satisfied: distro in /usr/local/lib/python3.12/dist-packages (from tabula-py) (1.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25.3->tabula-py) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25.3->tabula-py) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25.3->tabula-py) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->tabula-py) (1.17.0)\n",
            "Downloading tabula_py-2.10.0-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tabula-py\n",
            "Successfully installed tabula-py-2.10.0\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages (Run Once)\n",
        "\n",
        "!pip install pikepdf==8.3.0\n",
        "!pip install ocrmypdf\n",
        "!pip install PyMuPDF -q\n",
        "\n",
        "! apt install tesseract-ocr\n",
        "! apt install libtesseract-dev\n",
        "\n",
        "!pip install pytesseract\n",
        "!apt install ghostscript\n",
        "\n",
        "!pip install tabula-py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages\n",
        "import os\n",
        "import requests\n",
        "import ocrmypdf\n",
        "import subprocess # used to run ocrmypdf\n",
        "import shutil # used to replace original file with ocr-ed file\n",
        "import pymupdf # used to extract text from pdf\n",
        "import fitz # dependency for pymupdf\n",
        "import tabula # used to extract tables\n",
        "import pandas as pd # for data frame data structure, used for easy conversion to excel\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "yFyg6fyGnUxo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount to Google Drive (gives code access to files in Google Drive)\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "G5z1jyJ3tutq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7a4fb71-0e8d-40cf-faa0-130c72358b2a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up\n",
        "\n",
        "# Find path to folder with all pdfs\n",
        "folder_path = '/content/drive/My Drive/PDF_Text_Extraction_Project/PDFs'\n",
        "\n",
        "# Create an empty list to store texts extracted from pdf\n",
        "# Each element stores all the text for a pdf\n",
        "#pdf_texts = []\n"
      ],
      "metadata": {
        "id": "avEtlROdnctN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Java\n",
        "!apt-get update\n",
        "!apt-get install -y openjdk-11-jdk-headless\n",
        "\n",
        "# Set JAVA_HOME environment variable\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "# Verify installation\n",
        "!java -version\n",
        "\n",
        "# Now try tabula again\n",
        "import tabula\n",
        "\n",
        "folder_path = '/content/drive/My Drive/PDF_Text_Extraction_Project/PDFs'\n",
        "files = os.listdir(folder_path)\n",
        "\n",
        "if files:\n",
        "    test_file = os.path.join(folder_path, files[0])\n",
        "    print(f\"Processing: {test_file}\")\n",
        "\n",
        "    dfs = tabula.read_pdf(test_file, pages='1')\n",
        "    print(f\"Found {len(dfs)} tables\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4HO3ZTF-yAQY",
        "outputId": "11c2696d-a1fe-48c8-b4db-e8c70d78f92b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Waiting for headers] [W\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Waiting for headers] [W\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://cli.github.com/packages stable/main amd64 Packages [344 B]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,102 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,822 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,289 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,411 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,520 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,963 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,855 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,594 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,161 kB]\n",
            "Fetched 37.1 MB in 4s (10.3 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  ca-certificates-java java-common libpcsclite1 openjdk-11-jre-headless\n",
            "Suggested packages:\n",
            "  default-jre pcscd openjdk-11-demo openjdk-11-source libnss-mdns\n",
            "  fonts-dejavu-extra fonts-ipafont-gothic fonts-ipafont-mincho\n",
            "  fonts-wqy-microhei | fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  ca-certificates-java java-common libpcsclite1 openjdk-11-jdk-headless\n",
            "  openjdk-11-jre-headless\n",
            "0 upgraded, 5 newly installed, 0 to remove and 42 not upgraded.\n",
            "Need to get 116 MB of archives.\n",
            "After this operation, 258 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 java-common all 0.72build2 [6,782 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpcsclite1 amd64 1.9.5-3ubuntu1 [19.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre-headless amd64 11.0.28+6-1ubuntu1~22.04.1 [42.6 MB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ca-certificates-java all 20190909ubuntu1.2 [12.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk-headless amd64 11.0.28+6-1ubuntu1~22.04.1 [73.6 MB]\n",
            "Fetched 116 MB in 3s (46.0 MB/s)\n",
            "Selecting previously unselected package java-common.\n",
            "(Reading database ... 126317 files and directories currently installed.)\n",
            "Preparing to unpack .../java-common_0.72build2_all.deb ...\n",
            "Unpacking java-common (0.72build2) ...\n",
            "Selecting previously unselected package libpcsclite1:amd64.\n",
            "Preparing to unpack .../libpcsclite1_1.9.5-3ubuntu1_amd64.deb ...\n",
            "Unpacking libpcsclite1:amd64 (1.9.5-3ubuntu1) ...\n",
            "Selecting previously unselected package openjdk-11-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-11-jre-headless_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jre-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package ca-certificates-java.\n",
            "Preparing to unpack .../ca-certificates-java_20190909ubuntu1.2_all.deb ...\n",
            "Unpacking ca-certificates-java (20190909ubuntu1.2) ...\n",
            "Selecting previously unselected package openjdk-11-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-11-jdk-headless_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jdk-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Setting up java-common (0.72build2) ...\n",
            "Setting up libpcsclite1:amd64 (1.9.5-3ubuntu1) ...\n",
            "Setting up openjdk-11-jre-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jjs to provide /usr/bin/jjs (jjs) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmid to provide /usr/bin/rmid (rmid) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/pack200 to provide /usr/bin/pack200 (pack200) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/unpack200 to provide /usr/bin/unpack200 (unpack200) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode\n",
            "Setting up openjdk-11-jdk-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jar to provide /usr/bin/jar (jar) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jarsigner to provide /usr/bin/jarsigner (jarsigner) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javadoc to provide /usr/bin/javadoc (javadoc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javap to provide /usr/bin/javap (javap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jcmd to provide /usr/bin/jcmd (jcmd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdb to provide /usr/bin/jdb (jdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdeprscan to provide /usr/bin/jdeprscan (jdeprscan) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdeps to provide /usr/bin/jdeps (jdeps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jfr to provide /usr/bin/jfr (jfr) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jimage to provide /usr/bin/jimage (jimage) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jinfo to provide /usr/bin/jinfo (jinfo) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jlink to provide /usr/bin/jlink (jlink) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jmap to provide /usr/bin/jmap (jmap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jmod to provide /usr/bin/jmod (jmod) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jrunscript to provide /usr/bin/jrunscript (jrunscript) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jshell to provide /usr/bin/jshell (jshell) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstack to provide /usr/bin/jstack (jstack) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstat to provide /usr/bin/jstat (jstat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstatd to provide /usr/bin/jstatd (jstatd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmic to provide /usr/bin/rmic (rmic) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/serialver to provide /usr/bin/serialver (serialver) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jaotc to provide /usr/bin/jaotc (jaotc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jhsdb to provide /usr/bin/jhsdb (jhsdb) in auto mode\n",
            "Setting up ca-certificates-java (20190909ubuntu1.2) ...\n",
            "head: cannot open '/etc/ssl/certs/java/cacerts' for reading: No such file or directory\n",
            "Adding debian:AffirmTrust_Networking.pem\n",
            "Adding debian:DigiCert_Global_Root_G2.pem\n",
            "Adding debian:Certigna_Root_CA.pem\n",
            "Adding debian:Entrust_Root_Certification_Authority_-_EC1.pem\n",
            "Adding debian:Microsec_e-Szigno_Root_CA_2009.pem\n",
            "Adding debian:GlobalSign_Root_R46.pem\n",
            "Adding debian:Entrust_Root_Certification_Authority.pem\n",
            "Adding debian:DigiCert_Global_Root_CA.pem\n",
            "Adding debian:GDCA_TrustAUTH_R5_ROOT.pem\n",
            "Adding debian:QuoVadis_Root_CA_2_G3.pem\n",
            "Adding debian:GTS_Root_R2.pem\n",
            "Adding debian:emSign_Root_CA_-_C1.pem\n",
            "Adding debian:ANF_Secure_Server_Root_CA.pem\n",
            "Adding debian:OISTE_WISeKey_Global_Root_GC_CA.pem\n",
            "Adding debian:SwissSign_Silver_CA_-_G2.pem\n",
            "Adding debian:Autoridad_de_Certificacion_Firmaprofesional_CIF_A62634068.pem\n",
            "Adding debian:OISTE_WISeKey_Global_Root_GB_CA.pem\n",
            "Adding debian:Hongkong_Post_Root_CA_3.pem\n",
            "Adding debian:TUBITAK_Kamu_SM_SSL_Kok_Sertifikasi_-_Surum_1.pem\n",
            "Adding debian:Certum_Trusted_Network_CA_2.pem\n",
            "Adding debian:DigiCert_Assured_ID_Root_G3.pem\n",
            "Adding debian:DigiCert_Assured_ID_Root_CA.pem\n",
            "Adding debian:GLOBALTRUST_2020.pem\n",
            "Adding debian:GlobalSign_Root_CA_-_R6.pem\n",
            "Adding debian:DigiCert_High_Assurance_EV_Root_CA.pem\n",
            "Adding debian:IdenTrust_Commercial_Root_CA_1.pem\n",
            "Adding debian:Telia_Root_CA_v2.pem\n",
            "Adding debian:DigiCert_Assured_ID_Root_G2.pem\n",
            "Adding debian:Security_Communication_Root_CA.pem\n",
            "Adding debian:Comodo_AAA_Services_root.pem\n",
            "Adding debian:GlobalSign_ECC_Root_CA_-_R4.pem\n",
            "Adding debian:Hellenic_Academic_and_Research_Institutions_ECC_RootCA_2015.pem\n",
            "Adding debian:Starfield_Class_2_CA.pem\n",
            "Adding debian:Secure_Global_CA.pem\n",
            "Adding debian:Entrust_Root_Certification_Authority_-_G2.pem\n",
            "Adding debian:Entrust.net_Premium_2048_Secure_Server_CA.pem\n",
            "Adding debian:AC_RAIZ_FNMT-RCM_SERVIDORES_SEGUROS.pem\n",
            "Adding debian:Buypass_Class_3_Root_CA.pem\n",
            "Adding debian:Starfield_Services_Root_Certificate_Authority_-_G2.pem\n",
            "Adding debian:DigiCert_Global_Root_G3.pem\n",
            "Adding debian:SSL.com_EV_Root_Certification_Authority_ECC.pem\n",
            "Adding debian:Security_Communication_RootCA3.pem\n",
            "Adding debian:TeliaSonera_Root_CA_v1.pem\n",
            "Adding debian:Izenpe.com.pem\n",
            "Adding debian:Security_Communication_ECC_RootCA1.pem\n",
            "Adding debian:certSIGN_ROOT_CA.pem\n",
            "Adding debian:ISRG_Root_X2.pem\n",
            "Adding debian:USERTrust_RSA_Certification_Authority.pem\n",
            "Adding debian:Certigna.pem\n",
            "Adding debian:HiPKI_Root_CA_-_G1.pem\n",
            "Adding debian:ePKI_Root_Certification_Authority.pem\n",
            "Adding debian:DigiCert_TLS_RSA4096_Root_G5.pem\n",
            "Adding debian:Trustwave_Global_ECC_P256_Certification_Authority.pem\n",
            "Adding debian:TWCA_Root_Certification_Authority.pem\n",
            "Adding debian:D-TRUST_BR_Root_CA_1_2020.pem\n",
            "Adding debian:Baltimore_CyberTrust_Root.pem\n",
            "Adding debian:USERTrust_ECC_Certification_Authority.pem\n",
            "Adding debian:QuoVadis_Root_CA_1_G3.pem\n",
            "Adding debian:Microsoft_ECC_Root_Certificate_Authority_2017.pem\n",
            "Adding debian:GlobalSign_ECC_Root_CA_-_R5.pem\n",
            "Adding debian:CA_Disig_Root_R2.pem\n",
            "Adding debian:T-TeleSec_GlobalRoot_Class_2.pem\n",
            "Adding debian:Certainly_Root_R1.pem\n",
            "Adding debian:XRamp_Global_CA_Root.pem\n",
            "Adding debian:AffirmTrust_Premium_ECC.pem\n",
            "Adding debian:SecureSign_RootCA11.pem\n",
            "Adding debian:COMODO_RSA_Certification_Authority.pem\n",
            "Adding debian:Trustwave_Global_ECC_P384_Certification_Authority.pem\n",
            "Adding debian:D-TRUST_Root_Class_3_CA_2_EV_2009.pem\n",
            "Adding debian:e-Szigno_Root_CA_2017.pem\n",
            "Adding debian:certSIGN_Root_CA_G2.pem\n",
            "Adding debian:T-TeleSec_GlobalRoot_Class_3.pem\n",
            "Adding debian:Amazon_Root_CA_1.pem\n",
            "Adding debian:Starfield_Root_Certificate_Authority_-_G2.pem\n",
            "Adding debian:Amazon_Root_CA_2.pem\n",
            "Adding debian:Certum_EC-384_CA.pem\n",
            "Adding debian:HARICA_TLS_ECC_Root_CA_2021.pem\n",
            "Adding debian:Certainly_Root_E1.pem\n",
            "Adding debian:Hellenic_Academic_and_Research_Institutions_RootCA_2015.pem\n",
            "Adding debian:Atos_TrustedRoot_2011.pem\n",
            "Adding debian:Go_Daddy_Class_2_CA.pem\n",
            "Adding debian:DigiCert_Trusted_Root_G4.pem\n",
            "Adding debian:IdenTrust_Public_Sector_Root_CA_1.pem\n",
            "Adding debian:NetLock_Arany_=Class_Gold=_Főtanúsítvány.pem\n",
            "Adding debian:emSign_ECC_Root_CA_-_G3.pem\n",
            "Adding debian:GlobalSign_Root_E46.pem\n",
            "Adding debian:ACCVRAIZ1.pem\n",
            "Adding debian:Amazon_Root_CA_4.pem\n",
            "Adding debian:AffirmTrust_Commercial.pem\n",
            "Adding debian:Actalis_Authentication_Root_CA.pem\n",
            "Adding debian:Go_Daddy_Root_Certificate_Authority_-_G2.pem\n",
            "Adding debian:GTS_Root_R1.pem\n",
            "Adding debian:Certum_Trusted_Root_CA.pem\n",
            "Adding debian:CFCA_EV_ROOT.pem\n",
            "Adding debian:COMODO_Certification_Authority.pem\n",
            "Adding debian:Trustwave_Global_Certification_Authority.pem\n",
            "Adding debian:SSL.com_Root_Certification_Authority_ECC.pem\n",
            "Adding debian:SwissSign_Gold_CA_-_G2.pem\n",
            "Adding debian:NAVER_Global_Root_Certification_Authority.pem\n",
            "Adding debian:Buypass_Class_2_Root_CA.pem\n",
            "Adding debian:Certum_Trusted_Network_CA.pem\n",
            "Adding debian:Entrust_Root_Certification_Authority_-_G4.pem\n",
            "Adding debian:SecureTrust_CA.pem\n",
            "Adding debian:COMODO_ECC_Certification_Authority.pem\n",
            "Adding debian:UCA_Global_G2_Root.pem\n",
            "Adding debian:SZAFIR_ROOT_CA2.pem\n",
            "Adding debian:vTrus_ECC_Root_CA.pem\n",
            "Adding debian:Amazon_Root_CA_3.pem\n",
            "Adding debian:GTS_Root_R4.pem\n",
            "Adding debian:Security_Communication_RootCA2.pem\n",
            "Adding debian:emSign_Root_CA_-_G1.pem\n",
            "Adding debian:TWCA_Global_Root_CA.pem\n",
            "Adding debian:SSL.com_Root_Certification_Authority_RSA.pem\n",
            "Adding debian:HARICA_TLS_RSA_Root_CA_2021.pem\n",
            "Adding debian:TunTrust_Root_CA.pem\n",
            "Adding debian:SSL.com_EV_Root_Certification_Authority_RSA_R2.pem\n",
            "Adding debian:UCA_Extended_Validation_Root.pem\n",
            "Adding debian:QuoVadis_Root_CA_3.pem\n",
            "Adding debian:QuoVadis_Root_CA_2.pem\n",
            "Adding debian:D-TRUST_Root_Class_3_CA_2_2009.pem\n",
            "Adding debian:Microsoft_RSA_Root_Certificate_Authority_2017.pem\n",
            "Adding debian:AffirmTrust_Premium.pem\n",
            "Adding debian:ISRG_Root_X1.pem\n",
            "Adding debian:GTS_Root_R3.pem\n",
            "Adding debian:GlobalSign_Root_CA.pem\n",
            "Adding debian:vTrus_Root_CA.pem\n",
            "Adding debian:QuoVadis_Root_CA_3_G3.pem\n",
            "Adding debian:DigiCert_TLS_ECC_P384_Root_G5.pem\n",
            "Adding debian:AC_RAIZ_FNMT-RCM.pem\n",
            "Adding debian:D-TRUST_EV_Root_CA_1_2020.pem\n",
            "Adding debian:emSign_ECC_Root_CA_-_C3.pem\n",
            "Adding debian:GlobalSign_Root_CA_-_R3.pem\n",
            "Adding debian:CommScope_Public_Trust_ECC_Root-01.pem\n",
            "Adding debian:CommScope_Public_Trust_RSA_Root-02.pem\n",
            "Adding debian:Atos_TrustedRoot_Root_CA_RSA_TLS_2021.pem\n",
            "Adding debian:SSL.com_TLS_ECC_Root_CA_2022.pem\n",
            "Adding debian:Sectigo_Public_Server_Authentication_Root_E46.pem\n",
            "Adding debian:BJCA_Global_Root_CA1.pem\n",
            "Adding debian:TrustAsia_Global_Root_CA_G4.pem\n",
            "Adding debian:CommScope_Public_Trust_RSA_Root-01.pem\n",
            "Adding debian:Sectigo_Public_Server_Authentication_Root_R46.pem\n",
            "Adding debian:Atos_TrustedRoot_Root_CA_ECC_TLS_2021.pem\n",
            "Adding debian:CommScope_Public_Trust_ECC_Root-02.pem\n",
            "Adding debian:TrustAsia_Global_Root_CA_G3.pem\n",
            "Adding debian:SSL.com_TLS_RSA_Root_CA_2022.pem\n",
            "Adding debian:BJCA_Global_Root_CA2.pem\n",
            "done.\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for ca-certificates (20240203~22.04.1) ...\n",
            "Updating certificates in /etc/ssl/certs...\n",
            "0 added, 0 removed; done.\n",
            "Running hooks in /etc/ca-certificates/update.d...\n",
            "\n",
            "done.\n",
            "done.\n",
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tabula.backend:Failed to import jpype dependencies. Fallback to subprocess.\n",
            "WARNING:tabula.backend:No module named 'jpype'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: /content/drive/My Drive/PDF_Text_Extraction_Project/PDFs/TestDoc1.pdf\n",
            "Found 1 tables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Troubleshooting: Checking if folder path is correct\n",
        "\n",
        "def path_exists(f_path):\n",
        "  if os.path.exists(f_path):\n",
        "    print(\"Directory now exists!\")\n",
        "    return True\n",
        "  else:\n",
        "    print(\"Failed to create directory\")\n",
        "    return False"
      ],
      "metadata": {
        "id": "chX6ifwB0Wfr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_exists(folder_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "943sFMXVxm4t",
        "outputId": "f66ea485-6eb0-40de-e781-c7688bfa1716"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory now exists!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finds the last page number of a table\n",
        "# Inputs: folder_p <-- folder path, file_i <-- file index (used for looping), start_p <-- start page number\n",
        "# Returns an integer: the lasst page number a table extends across\n",
        "def find_last_page(folder_p, file_i, start_p):\n",
        "  print(\"Running find_last_page()\")\n",
        "  #print(folder_p, file_i, start_p)\n",
        "\n",
        "  # Gets all the files in a given folder\n",
        "  files = os.listdir(folder_p)\n",
        "\n",
        "  # Initialize variables\n",
        "  page_num = start_p\n",
        "  final_page = False\n",
        "\n",
        "  # If the folder is not empty\n",
        "  if files:\n",
        "    # Get the file\n",
        "    file = os.path.join(folder_p, files[file_i])\n",
        "    print(f\"Processing: {file}\")\n",
        "\n",
        "    # While not on the final page\n",
        "    # Increment and keep track of the page number\n",
        "    while final_page == False:\n",
        "\n",
        "      # Grabs the current page's table temporarily to scan info and check if it's last page\n",
        "      temp_dfs = tabula.read_pdf(file, lattice=True, pages=page_num)\n",
        "      pos1 = temp_dfs[0].iloc[-1,0] # Last row, first column\n",
        "      pos2 = temp_dfs[0].iloc[-1, 1] # Last row, second column\n",
        "\n",
        "      # Check if the table has ended by seeing if somewhere in the last row\n",
        "      # the word 'total' is seen\n",
        "      if pos1 == 'Total' or pos2 == 'Total':\n",
        "        final_page = True\n",
        "        break\n",
        "\n",
        "      page_num += 1\n",
        "\n",
        "    print(\"Final page?\", final_page)\n",
        "    print(\"Last page:\", page_num)\n",
        "\n",
        "    return page_num\n"
      ],
      "metadata": {
        "id": "facyXJme3Dpz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracts table data and stores it as a list of Pandas dataframes\n",
        "# First call on find_last_page() to see how many pages the table extends across\n",
        "# Then, extract table data within the page range\n",
        "# Returns a list of Pandas dataframes (for later merging)\n",
        "# Inputs: folder_p <-- folder path, file_i <-- file index (used for looping), start_p <-- start page number\n",
        "def extract_tables(folder_p, file_i, start_p):\n",
        "  print(\"Running extract_tables()\")\n",
        "\n",
        "  # Gets all the files in a given folder\n",
        "  files = os.listdir(folder_p)\n",
        "\n",
        "  # If folder is not empty\n",
        "  if files:\n",
        "    # Get the file and page_range\n",
        "    file = os.path.join(folder_p, files[file_i])\n",
        "    last_page_num = find_last_page(folder_p, file_i, start_p)\n",
        "\n",
        "    page_range = f'{start_p}-{last_page_num}'\n",
        "\n",
        "    # Extracts the text data on each page the table is in\n",
        "    dfs = tabula.read_pdf(file, lattice=True, pages=page_range)\n",
        "\n",
        "    print(\"Tables collected:\", len(dfs))\n",
        "\n",
        "    return dfs\n"
      ],
      "metadata": {
        "id": "9mia5sFA4eEj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixes the misprocessing where first row becomes column names\n",
        "# Grabs the first row data from the column names and make it a mergable row\n",
        "# Then, return the row\n",
        "def get_first_row(df):\n",
        "  print(\"Running get_first_row()\")\n",
        "\n",
        "  # Create the first row as a list: the name, followed by empty cells\n",
        "  first_row = [df.columns[0]]\n",
        "  first_row.extend([\"\"] * (df.shape[1] - 1))\n",
        "\n",
        "  # Turn it into a mergable row (dataframe type)\n",
        "  first_row_df = pd.DataFrame([first_row], columns=df.columns)\n",
        "\n",
        "  return first_row_df\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "36pGcIYF5yAa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix second row to repeat values for subcells.\n",
        "# Ex: Recruiting Agency, Recruiting Agency, Recruiting agency, Individual-New, ...\n",
        "# Then, returns the row\n",
        "def get_second_row(df):\n",
        "\n",
        "  print(\"Running get_second_row()\")\n",
        "  print(\"Dataframe size:\", df.shape)\n",
        "\n",
        "  # Copy the values over from the second row\n",
        "  second_row_copy = df.iloc[0]\n",
        "\n",
        "  # Drop 'nan' values\n",
        "  cleaned_second_row = second_row_copy.dropna()\n",
        "\n",
        "  # Create the second row as a list\n",
        "  second_row_data = cleaned_second_row.tolist()\n",
        "  print(second_row_data) # Debugging statement\n",
        "\n",
        "  # List form of the second row\n",
        "  second_row_list = [\"\", \"\"]\n",
        "\n",
        "  # Get each cell value in the original second row and write it 3 times\n",
        "  # in the new second row (this one will replace the original second row)\n",
        "  for value in second_row_data:\n",
        "    second_row_list.extend([value] * 3)\n",
        "    #second_row_list.extend([\"\"]) # Temp line of code: ensure column sizes match\n",
        "\n",
        "  print(\"Length of second_row_list\", len(second_row_list))\n",
        "\n",
        "  # Turn the list back into a dataframe\n",
        "  second_row_df = pd.DataFrame([second_row_list], columns=df.columns)\n",
        "\n",
        "  return second_row_df"
      ],
      "metadata": {
        "id": "lpHccoVAjxNP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixes column misalignment with 'total' in the last row\n",
        "# Returns a properly reformatted df (last dataframe in the list)\n",
        "def realign_last_row(df):\n",
        "\n",
        "  print(\"Running realign_last_row()\")\n",
        "\n",
        "  print(\"df shape:\", df.shape)\n",
        "\n",
        "  # Copy data from last row of data frame\n",
        "  # Drop any weird values (e.g. null values)\n",
        "  # Transform it into a list (lists are easy to work with)\n",
        "  # Shift 'total' one column to the right and make necessary adjustments\n",
        "  # Convert last row back to dataframe\n",
        "  # Then replace the original last row with the reformatted last row\n",
        "  last_row_copy = df.iloc[-1]\n",
        "\n",
        "  cleaned_last_row = last_row_copy.dropna()\n",
        "\n",
        "  last_row_list = cleaned_last_row.tolist()\n",
        "\n",
        "  # Reformatting\n",
        "  last_row_list.insert(0, \"\")\n",
        "  last_row_list.append(\"\")  # Add another blank element at the end so that column numbers match\n",
        "\n",
        "  print(last_row_list)\n",
        "  print(\"Length of last row list:\", len(last_row_list))\n",
        "\n",
        "  # Convert back to dataframe\n",
        "  last_row_df = pd.DataFrame([last_row_list], columns=df.columns)\n",
        "\n",
        "  # Replace by dropping the last row and replacing with our reformatted last row\n",
        "  df.drop(df.tail(1).index, inplace=True)\n",
        "\n",
        "  last_df_final = pd.concat([df, last_row_df], ignore_index=True)\n",
        "\n",
        "  return last_df_final\n",
        "\n"
      ],
      "metadata": {
        "id": "vBDKM0Gv7RA5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gets a properly merged and formatted dataframe based on the file and start page\n",
        "# Inputs: folder_p <-- folder path, file_i <-- file index (used for looping), start_p <-- start page number,\n",
        "#         one_page <-- bolean whether the table is only on one page\n",
        "def get_merged_df(folder_p, file_i, start_p, one_page):\n",
        "  print(\"Running get_merged_df()\")\n",
        "\n",
        "  if one_page:\n",
        "    df_list = extract_tables(folder_p, file_i, start_p)\n",
        "\n",
        "    first_row = get_first_row(df_list[0])\n",
        "    second_row = get_second_row(df_list[0])\n",
        "    remaining_first_table = df_list[0].iloc[1:]\n",
        "\n",
        "    df_merge_list = [first_row, second_row, remaining_first_table]\n",
        "\n",
        "  else:\n",
        "\n",
        "    # First get a table (likely spread across multiple pages)\n",
        "    # Store this as a list where each element is the part of the table on each page\n",
        "    df_list = extract_tables(folder_p, file_i, start_p)\n",
        "\n",
        "    # Get the properly formatted first and second rows\n",
        "    first_row = get_first_row(df_list[0])\n",
        "    second_row = get_second_row(df_list[0])\n",
        "\n",
        "    # We don't need the first two rows anymore so just grab the remaining rows\n",
        "    # from the first table\n",
        "    remaining_first_table = df_list[0].iloc[1:]\n",
        "\n",
        "    # Fix the column alignment in the last page with 'total' as well\n",
        "    # Assume the last row is misaligned\n",
        "    try:\n",
        "      last_table = realign_last_row(df_list[-1])\n",
        "      fixed_last_table = last_table.iloc[2:]\n",
        "    # The last row was actually properly aligned\n",
        "    except:\n",
        "      last_table = df_list[-1]\n",
        "      fixed_last_table = last_table.iloc[2:]\n",
        "\n",
        "    # Now we are ready to merge these dataframes together as a singular table\n",
        "    # Start with the first table which includes: properly formatted first two rows + remaining rows\n",
        "    df_merge_list = [first_row, second_row, remaining_first_table]\n",
        "\n",
        "    # Now loop through other pages and append those first\n",
        "    # Start at the second page (index 1) because we have the first page data already\n",
        "    # End at the second last page (index length - 1) because we have fixed_last_table\n",
        "    for i in range(1, len(df_list) - 1):\n",
        "      df = df_list[i].iloc[2:]\n",
        "      df_merge_list.append(df)\n",
        "\n",
        "    # Once all other pages are handled, append table from the last page\n",
        "    df_merge_list.append(fixed_last_table)\n",
        "\n",
        "  # Once everything (e.g. pages) is ordered correctly, append the separate dataframes\n",
        "  final_df = pd.concat(df_merge_list)\n",
        "\n",
        "  return final_df\n",
        "\n"
      ],
      "metadata": {
        "id": "_g9fLbpWk7oy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes a dataframe data structure and exports it to a csv file\n",
        "# Stores it in whatever file path specified (currently a google drive folder)\n",
        "# Inputs: df <-- the dataframe, name <-- what you want to name the file\n",
        "def export_file_to_csv(df, name):\n",
        "\n",
        "  print(\"Running export_file_to_csv()\")\n",
        "\n",
        "  root_store_path = '/content/drive/My Drive/PDF_Text_Extraction_Project/Excel_Tables'\n",
        "  store_path = os.path.join(root_store_path, f\"{name}.csv\")\n",
        "\n",
        "  df.to_csv(store_path)"
      ],
      "metadata": {
        "id": "pf7ykuImsbJQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop function\n",
        "\n",
        "def main(folder_p):\n",
        "\n",
        "  # Gets all the files in a given folder\n",
        "  files = os.listdir(folder_p)\n",
        "\n",
        "  # If folder is not empty\n",
        "  if files:\n",
        "    # i now acts as the file index in the folder\n",
        "    for i in range(len(files)):\n",
        "      print(\"Processing file:\", files[i])\n",
        "\n",
        "      ## For Country-wise Table\n",
        "      start_page = 1\n",
        "      one_page = False\n",
        "\n",
        "      # First check if the table is only on one page\n",
        "      last_page = find_last_page(folder_p, i, start_page)\n",
        "      if start_page == last_page:\n",
        "        one_page = True\n",
        "\n",
        "      country_df = get_merged_df(folder_p, i, start_page, one_page)\n",
        "      temp_file_name = files[i].split('.')\n",
        "      file_name = temp_file_name[0] + \"_country\"\n",
        "      export_file_to_csv(country_df, file_name)\n",
        "\n",
        "      # Reset the one_page variable\n",
        "      one_page = False\n",
        "      # Update the start page to be the next page from where we left off\n",
        "      start_page = last_page + 1\n",
        "\n",
        "      ## For District-wise Table\n",
        "      # First check if the table is only on one page\n",
        "      # last_page = find_last_page(folder_p, i, start_page)\n",
        "      # if start_page == last_page:\n",
        "      #   one_page = True\n",
        "\n",
        "      # district_df = get_merged_df(folder_p, i, start_page, one_page)\n",
        "      # temp_file_name = files[i].split('.')\n",
        "      # file_name = temp_file_name[0] + \"_district\"\n",
        "      # export_file_to_csv(district_df, file_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xcCsRgr6s8vx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = main(folder_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jIT-PVZhuOTT",
        "outputId": "2f98b28d-0037-44e2-fd14-e003e6aa5ad1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: TestDoc1.pdf\n",
            "Running find_last_page()\n",
            "Processing: /content/drive/My Drive/PDF_Text_Extraction_Project/PDFs/TestDoc1.pdf\n",
            "Final page? True\n",
            "Last page: 1\n",
            "Running get_merged_df()\n",
            "Running extract_tables()\n",
            "Running find_last_page()\n",
            "Processing: /content/drive/My Drive/PDF_Text_Extraction_Project/PDFs/TestDoc1.pdf\n",
            "Final page? True\n",
            "Last page: 1\n",
            "Tables collected: 1\n",
            "Running get_first_row()\n",
            "Running get_second_row()\n",
            "Dataframe size: (101, 23)\n",
            "['Recruiting Agency', 'Individual-New', 'G-to-G', 'Individual-ReEntry', 'Legalization', 'Total with ReEntry', 'Total without ReEntry']\n",
            "Length of second_row_list 23\n",
            "Running export_file_to_csv()\n",
            "Processing file: TestDoc2.pdf\n",
            "Running find_last_page()\n",
            "Processing: /content/drive/My Drive/PDF_Text_Extraction_Project/PDFs/TestDoc2.pdf\n",
            "Final page? True\n",
            "Last page: 2\n",
            "Running get_merged_df()\n",
            "Running extract_tables()\n",
            "Running find_last_page()\n",
            "Processing: /content/drive/My Drive/PDF_Text_Extraction_Project/PDFs/TestDoc2.pdf\n",
            "Final page? True\n",
            "Last page: 2\n",
            "Tables collected: 2\n",
            "Running get_first_row()\n",
            "Running get_second_row()\n",
            "Dataframe size: (92, 23)\n",
            "['Recruiting Agency', 'Individual-New', 'G-to-G', 'Individual-ReEntry', 'Legalization', 'Total with ReEntry', 'Total without ReEntry']\n",
            "Length of second_row_list 23\n",
            "Running realign_last_row()\n",
            "df shape: (18, 24)\n",
            "['', 'Total', '29130', '2700', '31830', '8535', '3217', '11752', '724', '140', '864', '20547', '3097', '23644', '12', '8', '20', '58948', '9162', '68110', '38401', '6065', '44466', '']\n",
            "Length of last row list: 24\n",
            "Running export_file_to_csv()\n",
            "Processing file: TestDoc3.pdf\n",
            "Running find_last_page()\n",
            "Processing: /content/drive/My Drive/PDF_Text_Extraction_Project/PDFs/TestDoc3.pdf\n",
            "Final page? True\n",
            "Last page: 2\n",
            "Running get_merged_df()\n",
            "Running extract_tables()\n",
            "Running find_last_page()\n",
            "Processing: /content/drive/My Drive/PDF_Text_Extraction_Project/PDFs/TestDoc3.pdf\n",
            "Final page? True\n",
            "Last page: 2\n",
            "Tables collected: 2\n",
            "Running get_first_row()\n",
            "Running get_second_row()\n",
            "Dataframe size: (89, 23)\n",
            "['Recruiting Agency', 'Individual-New', 'G-to-G', 'Individual-ReEntry', 'Legalization', 'Total with ReEntry', 'Total without ReEntry']\n",
            "Length of second_row_list 23\n",
            "Running realign_last_row()\n",
            "df shape: (16, 23)\n",
            "['', 'Total', '36007', '3092', '39099', '9881', '3446', '13327', '1033', '186', '1219', '22746', '2488', '25234', '1', '0', '1', '69668', '9212', '78880', '46922', '6724', '53646', '']\n",
            "Length of last row list: 24\n",
            "Running export_file_to_csv()\n",
            "Processing file: Document_2025061605160.pdf\n",
            "Running find_last_page()\n",
            "Processing: /content/drive/My Drive/PDF_Text_Extraction_Project/PDFs/Document_2025061605160.pdf\n",
            "Final page? True\n",
            "Last page: 2\n",
            "Running get_merged_df()\n",
            "Running extract_tables()\n",
            "Running find_last_page()\n",
            "Processing: /content/drive/My Drive/PDF_Text_Extraction_Project/PDFs/Document_2025061605160.pdf\n",
            "Final page? True\n",
            "Last page: 2\n",
            "Tables collected: 2\n",
            "Running get_first_row()\n",
            "Running get_second_row()\n",
            "Dataframe size: (89, 23)\n",
            "['Recruiting Agency', 'Individual-New', 'G-to-G', 'Individual-ReEntry', 'Legalization', 'Total with ReEntry', 'Total without ReEntry']\n",
            "Length of second_row_list 23\n",
            "Running realign_last_row()\n",
            "df shape: (21, 23)\n",
            "['', 'Total', '30922', '2716', '33638', '8860', '3045', '11905', '1057', '114', '1171', '25463', '2290', '27753', '0', '0', '0', '66302', '8165', '74467', '40839', '5875', '46714', '']\n",
            "Length of last row list: 24\n",
            "Running export_file_to_csv()\n",
            "Processing file: Document_2025052704090.pdf\n",
            "Running find_last_page()\n",
            "Processing: /content/drive/My Drive/PDF_Text_Extraction_Project/PDFs/Document_2025052704090.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tabula.backend:Got stderr: Nov 04, 2025 11:43:00 PM org.apache.pdfbox.pdmodel.font.FileSystemFontProvider loadDiskCache\n",
            "WARNING: New fonts found, font cache will be re-built\n",
            "Nov 04, 2025 11:43:00 PM org.apache.pdfbox.pdmodel.font.FileSystemFontProvider <init>\n",
            "WARNING: Building on-disk font cache, this may take a while\n",
            "Nov 04, 2025 11:43:00 PM org.apache.pdfbox.pdmodel.font.FileSystemFontProvider <init>\n",
            "WARNING: Finished building on-disk font cache, found 56 fonts\n",
            "Nov 04, 2025 11:43:00 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "Nov 04, 2025 11:43:01 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans-Bold' for 'TimesNewRomanPS-BoldMT'\n",
            "\n",
            "WARNING:tabula.backend:Got stderr: Nov 04, 2025 11:43:04 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans-Bold' for 'TimesNewRomanPS-BoldMT'\n",
            "Nov 04, 2025 11:43:05 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final page? True\n",
            "Last page: 2\n",
            "Running get_merged_df()\n",
            "Running extract_tables()\n",
            "Running find_last_page()\n",
            "Processing: /content/drive/My Drive/PDF_Text_Extraction_Project/PDFs/Document_2025052704090.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tabula.backend:Got stderr: Nov 04, 2025 11:43:08 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "Nov 04, 2025 11:43:08 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans-Bold' for 'TimesNewRomanPS-BoldMT'\n",
            "\n",
            "WARNING:tabula.backend:Got stderr: Nov 04, 2025 11:43:12 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans-Bold' for 'TimesNewRomanPS-BoldMT'\n",
            "Nov 04, 2025 11:43:12 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final page? True\n",
            "Last page: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tabula.backend:Got stderr: Nov 04, 2025 11:43:14 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "Nov 04, 2025 11:43:14 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans-Bold' for 'TimesNewRomanPS-BoldMT'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tables collected: 2\n",
            "Running get_first_row()\n",
            "Running get_second_row()\n",
            "Dataframe size: (89, 23)\n",
            "['Recruiting Agency', 'Individual-New', 'G-to-G', 'Individual-ReEntry', 'Legalization', 'Total with ReEntry', 'Total without ReEntry']\n",
            "Length of second_row_list 23\n",
            "Running realign_last_row()\n",
            "df shape: (20, 23)\n",
            "['', 'Total', '30089', '2922', '33011', '10167', '3201', '13368', '867', '142', '1009', '27996', '2666', '30662', '0', '0', '0', '69119', '8931', '78050', '41123', '6265', '47388', '']\n",
            "Length of last row list: 24\n",
            "Running export_file_to_csv()\n",
            "Processing file: Document_2025022306410.pdf\n",
            "Running find_last_page()\n",
            "Processing: /content/drive/My Drive/PDF_Text_Extraction_Project/PDFs/Document_2025022306410.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tabula.backend:Got stderr: Nov 04, 2025 11:43:19 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans-Bold' for 'TimesNewRomanPS-BoldMT'\n",
            "Nov 04, 2025 11:43:19 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "Nov 04, 2025 11:43:19 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "\n",
            "WARNING:tabula.backend:Got stderr: Nov 04, 2025 11:43:23 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans-Bold' for 'TimesNewRomanPS-BoldMT'\n",
            "Nov 04, 2025 11:43:23 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "Nov 04, 2025 11:43:23 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final page? True\n",
            "Last page: 2\n",
            "Running get_merged_df()\n",
            "Running extract_tables()\n",
            "Running find_last_page()\n",
            "Processing: /content/drive/My Drive/PDF_Text_Extraction_Project/PDFs/Document_2025022306410.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tabula.backend:Got stderr: Nov 04, 2025 11:43:25 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans-Bold' for 'TimesNewRomanPS-BoldMT'\n",
            "Nov 04, 2025 11:43:25 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "Nov 04, 2025 11:43:25 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "\n",
            "WARNING:tabula.backend:Got stderr: Nov 04, 2025 11:43:28 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans-Bold' for 'TimesNewRomanPS-BoldMT'\n",
            "Nov 04, 2025 11:43:28 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "Nov 04, 2025 11:43:28 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final page? True\n",
            "Last page: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tabula.backend:Got stderr: Nov 04, 2025 11:43:31 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans-Bold' for 'TimesNewRomanPS-BoldMT'\n",
            "Nov 04, 2025 11:43:31 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "Nov 04, 2025 11:43:31 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tables collected: 2\n",
            "Running get_first_row()\n",
            "Running get_second_row()\n",
            "Dataframe size: (79, 20)\n",
            "['Recruiting Agency', 'Individual-New', 'G-to-G', 'Individual-ReEntry', 'Total with ReEntry', 'Total without ReEntry']\n",
            "Length of second_row_list 20\n",
            "Running realign_last_row()\n",
            "df shape: (29, 20)\n",
            "['', 'Total', '30059', '3393', '33452', '7564', '2151', '9715', '959', '57', '1016', '25729', '2529', '28258', '64311', '8130', '72441', '38582', '5601', '44183', '']\n",
            "Length of last row list: 21\n",
            "Running export_file_to_csv()\n",
            "Processing file: Document_2025011903430.pdf\n",
            "Running find_last_page()\n",
            "Processing: /content/drive/My Drive/PDF_Text_Extraction_Project/PDFs/Document_2025011903430.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tabula.backend:Got stderr: Nov 04, 2025 11:43:36 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "Nov 04, 2025 11:43:36 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "Nov 04, 2025 11:43:36 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans-Bold' for 'TimesNewRomanPS-BoldMT'\n",
            "\n",
            "WARNING:tabula.backend:Got stderr: Nov 04, 2025 11:43:38 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans-Bold' for 'TimesNewRomanPS-BoldMT'\n",
            "Nov 04, 2025 11:43:39 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "Nov 04, 2025 11:43:39 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final page? True\n",
            "Last page: 2\n",
            "Running get_merged_df()\n",
            "Running extract_tables()\n",
            "Running find_last_page()\n",
            "Processing: /content/drive/My Drive/PDF_Text_Extraction_Project/PDFs/Document_2025011903430.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tabula.backend:Got stderr: Nov 04, 2025 11:43:41 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "Nov 04, 2025 11:43:41 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "Nov 04, 2025 11:43:41 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans-Bold' for 'TimesNewRomanPS-BoldMT'\n",
            "\n",
            "WARNING:tabula.backend:Got stderr: Nov 04, 2025 11:43:44 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans-Bold' for 'TimesNewRomanPS-BoldMT'\n",
            "Nov 04, 2025 11:43:44 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "Nov 04, 2025 11:43:44 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final page? True\n",
            "Last page: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tabula.backend:Got stderr: Nov 04, 2025 11:43:47 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "Nov 04, 2025 11:43:47 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans' for 'TimesNewRomanPSMT'\n",
            "Nov 04, 2025 11:43:47 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
            "WARNING: Using fallback font 'LiberationSans-Bold' for 'TimesNewRomanPS-BoldMT'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tables collected: 2\n",
            "Running get_first_row()\n",
            "Running get_second_row()\n",
            "Dataframe size: (79, 20)\n",
            "['Recruiting Agency', 'Individual-New', 'G-to-G', 'Individual-ReEntry', 'Total with ReEntry', 'Total without ReEntry']\n",
            "Length of second_row_list 20\n",
            "Running realign_last_row()\n",
            "df shape: (26, 20)\n",
            "['', 'Total', '26618', '2698', '29316', '7742', '2343', '10085', '619', '35', '654', '25174', '2029', '27203', '60153', '7105', '67258', '34979', '5076', '40055', '']\n",
            "Length of last row list: 21\n",
            "Running export_file_to_csv()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_last_page = find_last_page(folder_path, 0, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev6H7Dc1w4wo",
        "outputId": "edb33652-57a8-4e22-a828-0ae51d63fa27"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running find_last_page()\n",
            "Processing: /content/drive/My Drive/PDF_Text_Extraction_Project/PDFs/TestDoc1.pdf\n",
            "Final page? True\n",
            "Last page: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def runocr(file):\n",
        "    \"\"\"Run OCR on a PDF file, replace it and rename with OCR_ prefix\"\"\"\n",
        "\n",
        "    # Extract directory and filename\n",
        "    directory = os.path.dirname(file)\n",
        "    original_filename = os.path.basename(file)\n",
        "\n",
        "    # Create new filename with OCR_ prefix\n",
        "    new_filename = \"OCR_\" + original_filename\n",
        "    new_file_path = os.path.join(directory, new_filename)\n",
        "\n",
        "    print(f\"Processing: {original_filename}\")\n",
        "    print(f\"Will rename to: {new_filename}\")\n",
        "\n",
        "    # Run OCR to temporary file first\n",
        "    temp_file = \"temp_ocr_output.pdf\"\n",
        "    result = subprocess.run(\n",
        "        ['ocrmypdf', '--force-ocr', file, temp_file],\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        # Remove original file\n",
        "        os.remove(file)\n",
        "        # Rename temporary file to new name\n",
        "        shutil.move(temp_file, new_file_path)\n",
        "        print(f\"✓ OCR successful! Renamed to: {new_filename}\")\n",
        "\n",
        "        # Open and return the renamed file\n",
        "        return_file = fitz.open(new_file_path)\n",
        "        return return_file\n",
        "    else:\n",
        "        print(\"✗ OCR failed! Return code:\", result.returncode)\n",
        "        if result.stderr:\n",
        "            print(\"Error:\", result.stderr)\n",
        "        # Clean up temporary file if it exists\n",
        "        if os.path.exists(temp_file):\n",
        "            os.remove(temp_file)\n",
        "        return None"
      ],
      "metadata": {
        "id": "iBhRIdNh1ePW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sys import ps1\n",
        "def get_pdf_text(pdf_file):\n",
        "\n",
        "  num_pages = pdf_file.page_count\n",
        "  pdf_text = \"\"\n",
        "  for i in range(0, num_pages):\n",
        "    page = pdf_file.load_page(i)\n",
        "    page_text = page.get_text()\n",
        "\n",
        "    pdf_text += page_text\n",
        "\n",
        "  return pdf_text"
      ],
      "metadata": {
        "id": "Y_0g2LbixFId"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_text_extraction(text_list):\n",
        "  files = os.listdir(folder_path)\n",
        "\n",
        "  if files:\n",
        "    for file in files:\n",
        "      if \"OCR\" not in file:\n",
        "        print(file, \"not yet OCR-ed\")\n",
        "        file_path = os.path.join(folder_path, file)\n",
        "        print(f\"Processing: {file_path}\")\n",
        "\n",
        "        my_ocr_file = runocr(file_path)\n",
        "\n",
        "        if my_ocr_file:\n",
        "          print(f\"Success! Processed PDF has {len(my_ocr_file)} pages\")\n",
        "          pdf_text = get_pdf_text(my_ocr_file)\n",
        "          text_list.append(pdf_text)\n",
        "        else:\n",
        "          print(\"OCR processing failed\")\n",
        "      else:\n",
        "        print(file, \"OCR-ed already\")\n",
        "        file_path = os.path.join(folder_path, file)\n",
        "        my_file = fitz.open(file_path)\n",
        "        pdf_text = get_pdf_text(my_file)\n",
        "        text_list.append(pdf_text)\n",
        "  else:\n",
        "    print(\"No files found\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2O4SzB5-nvWf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_file(url):\n",
        "    local_filename = url.split('/')[-1]\n",
        "\n",
        "    with requests.get(url) as r:\n",
        "        assert r.status_code == 200, f'error, status code is {r.status_code}'\n",
        "        with open(local_filename, 'wb') as f:\n",
        "            f.write(r.content)\n",
        "\n",
        "    return local_filename\n"
      ],
      "metadata": {
        "id": "LCaS7SqrpMhT"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}